{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6864, 0.9000, 0.8901, 0.7105, 0.7297, 0.7788, 0.9000, 0.8020,\n",
       "        0.8100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.preprocessing import compute_class_weight\n",
    "\n",
    "test_data = torch.randint(0, 10, (1000,))\n",
    "weights = compute_class_weight(test_data, 20)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UTRSegDecoder(\n",
       "  (clpse): ModuleList(\n",
       "    (0-3): 4 x ReduceTemporalFeature(\n",
       "      (depth_collapse): Sequential(\n",
       "        (0): Conv3d(768, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (pred): Identity()\n",
       "    )\n",
       "  )\n",
       "  (deconv): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): TRDeconv(\n",
       "        (deconv): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): TRSegBlock(\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (patch_expand): TRSegPatchExpand(\n",
       "        (upconv): Sequential(\n",
       "          (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TRSegBlock(\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (patch_expand): TRSegPatchExpand(\n",
       "        (upconv): Sequential(\n",
       "          (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TRSegBlock(\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=False)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (patch_expand): TRSegPatchExpand(\n",
       "        (upconv): Sequential(\n",
       "          (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pred): TRSegHead(\n",
       "    (pred): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.components.heads.utrseg import UTRSegDecoder\n",
    "\n",
    "trseg = UTRSegDecoder(768, 20, (3, 8, 8), [512, 256, 128, 64])\n",
    "trseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = [\n",
    "    torch.rand(1, 192, 768),\n",
    "    torch.rand(1, 192, 768),\n",
    "    torch.rand(1, 192, 768),\n",
    "    torch.rand(1, 192, 768)\n",
    "]\n",
    "output_test = trseg(input_test)\n",
    "output_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalAttentionDeconv(\n",
       "  (merge_attn): MergeAttentionBlock(\n",
       "    (attn): SelfAttentionLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeLayer(\n",
       "      (merge): Sequential(\n",
       "        (0): Conv3d(768, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
       "        (1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (pred): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (deconv): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (bottleneck): ModuleList(\n",
       "    (0): DeconvBottleneck(\n",
       "      (deconv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (1): DeconvBottleneck(\n",
       "      (deconv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (2): DeconvBottleneck(\n",
       "      (deconv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(768, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): ConvTranspose2d(128, 20, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (merge): ModuleList(\n",
       "    (0-2): 3 x MergeAttentionBlock(\n",
       "      (attn): SelfAttentionLayer(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (merge): MergeLayer(\n",
       "        (merge): Sequential(\n",
       "          (0): Conv3d(768, 768, kernel_size=(3, 1, 1), stride=(1, 1, 1))\n",
       "          (1): BatchNorm3d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (pred): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.components.heads.unetq import TemporalAttentionDeconv\n",
    "\n",
    "head = TemporalAttentionDeconv(768, 20, (3, 8, 8), [512, 256, 128])\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 128, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = [\n",
    "    torch.rand(1, 768, 192),\n",
    "    torch.rand(1, 768, 192),\n",
    "    torch.rand(1, 768, 192),\n",
    "    torch.rand(1, 768, 192),\n",
    "]\n",
    "head(test_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from timm.models.swin_transformer import (\n",
    "    SwinTransformerBlock\n",
    ")\n",
    "from typing import Literal\n",
    "\n",
    "class PatchExpanding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            input_resolution,\n",
    "            dim_scale=2, # type: Literal[0, 2]\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            norm_kwargs=None\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.expand = nn.Linear(dim, dim * dim_scale, bias=False) if dim_scale == 2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim * dim_scale, **({} if norm_kwargs is None else norm_kwargs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "\n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            input_resolution,\n",
    "            shift_size\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.ssbq = nn.Sequential(\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                input_resolution=input_resolution,\n",
    "                num_heads=1,\n",
    "                mlp_ratio=4.\n",
    "            ),\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                input_resolution=input_resolution,\n",
    "                num_heads=1,\n",
    "                shift_size=shift_size,\n",
    "                mlp_ratio=4.\n",
    "            )\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ssbq(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 6, 9],\n",
       "         [2, 5, 8],\n",
       "         [1, 4, 7]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.functional import random_rotation\n",
    "\n",
    "test = torch.tensor([[\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "    ]])\n",
    "test.shape\n",
    "\n",
    "random_rotation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test a data: tensor([1])\n",
      "None\n",
      "test b data: tensor([2])\n",
      "None\n",
      "(tensor([3]),)\n",
      "(tensor([4]),)\n"
     ]
    }
   ],
   "source": [
    "from src.data.transforms import CombinedRandomTransform\n",
    "\n",
    "\n",
    "def test_a(data: ...):\n",
    "    print(f\"test a data: {data}\")\n",
    "\n",
    "def test_b(data: ...):\n",
    "    print(f\"test b data: {data}\")\n",
    "\n",
    "transform = CombinedRandomTransform(test_a, test_b)\n",
    "print(transform(torch.tensor([1])))\n",
    "print(transform(torch.tensor([2])))\n",
    "print(transform(torch.tensor([3])))\n",
    "print(transform(torch.tensor([4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.data.transforms import loose_bind_transforms\n",
    "\n",
    "type(loose_bind_transforms(test_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Discarded kwarg: {'d'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.scripting import loose_bind_kwargs\n",
    "\n",
    "@loose_bind_kwargs(True)\n",
    "def fn(a, b, c):\n",
    "    return a + b + c\n",
    "\n",
    "kwarg = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2,\n",
    "    \"c\": 3,\n",
    "    \"d\": 4,\n",
    "}\n",
    "fn(**kwarg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('ibm-nasa-geospatial/Prithvi-100M', 'Prithvi_EO_V1_100M.pt'))\n",
      "c:\\Users\\EMC\\anaconda3\\envs\\sits-crop-recognition\\Lib\\site-packages\\timm\\models\\_hub.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(cached_file, map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelWiseModel(\n",
       "  (encoder): TimmBackboneWrapper(\n",
       "    (_timm_module): PrithviViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(6, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): FCNDecoder(\n",
       "    (convs): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(2304, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Norm2d(\n",
       "          (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Norm2d(\n",
       "          (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Norm2d(\n",
       "          (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Norm2d(\n",
       "          (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): SegmentationHead(\n",
       "    (head): Sequential(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (aux_heads): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from terratorch.models.encoder_decoder_factory import EncoderDecoderFactory\n",
    "from terratorch.datasets import HLSBands\n",
    "from terratorch.models.prithvi_model_factory import PrithviModelFactory\n",
    "\n",
    "factory = EncoderDecoderFactory()\n",
    "model = factory.build_model(\n",
    "    task=\"segmentation\",\n",
    "    backbone=\"prithvi_vit_100\",\n",
    "    decoder=\"FCNDecoder\",\n",
    "    num_classes=20,\n",
    "    backbone_in_chans=\"backbone_in_chans\",\n",
    "    backbone_pretrained=True,\n",
    "    backbone_num_frames=3,\n",
    "    backbone_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2\n",
    "    ],\n",
    "    necks=None\n",
    ")\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
